{"cells":[{"cell_type":"markdown","source":["# 1. Data aggregation, summarization and correlation"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"28807e26-c699-4bf9-b4c2-460c41a6cb6e"},{"cell_type":"markdown","source":["In this scenario, the data engineer could aggregate and summarize the data to provide insights into the overall trends and patterns in the dataset. For example, they could group the data by some columns (such as VendorID or RatecodeID) and calculate some summary statistics for the numerical columns (such as average fare_amount or total trip_distance). This could involve using Spark's built-in aggregation functions (such as groupBy and agg) to perform these calculations.\n","\n","The code calculates the average fare amount per month by grouping the DataFrame df by year and month of the lpep_pickup_datetime column. It uses the avg function from the [pyspark.sql.functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html) module to calculate the average fare amount and aliases the resulting column as \"average_fare\". The resulting DataFrame average_fare_per_month is sorted by year and month and is displayed using the display function. Finally, the code saves the results to a new delta table named \"average_fare_per_month\" using the write function with \"delta\" format, and \"overwrite\" mode.\n","\n","To execute the cell code, use the shortcut CTRL + Enter on Windows, or ⌘ + Enter on MacOS. Alternatively, you can click the 'Run' icon (▶️) located on the left side of the code cell."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b70dc9e6-638c-4633-b32e-1ec3465320ff"},{"cell_type":"code","source":["lakehouse_silver = 'silvercleansed_damian'\n","lakehouse_bronze = 'bronzerawdata_damian'"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0739500b-e113-46a4-8d91-2d201fc039ae"},{"cell_type":"markdown","source":["## Check the schema for two tables"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"11e77dd0-a5b6-41fe-b183-97ee9d047000"},{"cell_type":"code","source":["spark.read.table(\"green202301\").printSchema()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"46d05bef-848e-4b0f-8990-cd4f50ea0636"},{"cell_type":"code","source":["spark.read.table(\"green201501\").printSchema()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a286c2c5-b915-4505-aeab-f2c6b52a14cd"},{"cell_type":"markdown","source":["The schema is the same. We can just parametrize the notebook :) "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3819ff46-9c8c-461c-9027-b7af410ff2a6"},{"cell_type":"code","source":["table_name  = \"green201501\"\n","\n","data_collection = table_name[:-6]  # Extracts all characters except the last six (assumes these are non-digits)\n","extracted_year = table_name[-6:-2]  # Extracts the four digits representing the year\n","extracted_month = table_name[-2:]  # Extracts the last two digits representing the month"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"id":"785e4c8f-1ab0-4185-afa9-73dc3ea498e0"},{"cell_type":"code","source":["from pyspark.sql.functions import col, year, month, dayofmonth, avg\n","\n","df = spark.read.table(table_name)\n","\n","# Calculate average fare amount per month\n","average_fare_per_month = (\n","    df\n","    .groupBy(year(\"lpep_pickup_datetime\").alias(\"year\"), month(\"lpep_pickup_datetime\").alias(\"month\"))\n","    .agg(avg(\"fare_amount\").alias(\"average_fare\"))\n","    .orderBy(\"year\", \"month\")\n",")\n","display(average_fare_per_month)\n","\n","result_table_name = f\"{table_name}_avg_fare_per_month\"\n","\n","# Save the results to a new delta table\n","average_fare_per_month.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{lakehouse_silver}.{result_table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"pycharm":{"is_executing":true}},"id":"d96aa05c-d9be-472b-943e-1ba8812a24f6"},{"cell_type":"markdown","source":["### Refresh Lakehouse explorer"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"137647a8-70d6-4908-a384-3a2a39dcb0ea"},{"cell_type":"markdown","source":["## Exploratory data analysis (EDA)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"50f3624d-1190-4d94-bfad-74b71b27e50e"},{"cell_type":"markdown","source":["\n","\n","In this scenario, the data engineer could perform some data cleaning and transformation tasks to prepare the data for downstream analysis. \n","\n","Objective: **Cleanse the data and filter out invalid records for further analysis.**\n","\n","In this scenario, we aim to demonstrate how data engineers can perform data cleansing and filtering on a large dataset. We begin by loading the data from the source and then filter out records where the trip distance and fare amount are less than or equal to zero, which are invalid records.\n","\n","Next, we cleanse the data by converting the `store_and_fwd_flag` column to a boolean type, and converting the `lpep_pickup_datetime` and `lpep_dropoff_datetime` columns to timestamp types. Finally, we write the cleansed data to the destination in the parquet format.\n","\n","This scenario demonstrates the importance of data cleansing and filtering to ensure the data is accurate and valid before proceeding with further analysis."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3c1bca52-5ac6-47b7-a18d-3a65a6610afa"},{"cell_type":"code","source":["from pyspark.sql.functions import col, when\n","\n","# Load data from source\n","df = spark.read.load(f\"Tables/{table_name}\", header=True, inferSchema=True)\n","df_count = df.count()\n","\n","# Remove invalid records\n","df = df.filter((col(\"trip_distance\") > 0) & (col(\"fare_amount\") > 0))\n","df_count_after_clearning = df.count()\n","\n","number_of_deleted_records = df_count - df_count_after_clearning\n","\n","print(f\"Removed {number_of_deleted_records} records\")\n","\n","# # Cleanse data\n","df = df.withColumn(\"store_and_fwd_flag\", when(col(\"store_and_fwd_flag\") == \"Y\", True).otherwise(False))\n","df = df.withColumn(\"lpep_pickup_datetime\", col(\"lpep_pickup_datetime\").cast(\"timestamp\"))\n","df = df.withColumn(\"lpep_dropoff_datetime\", col(\"lpep_dropoff_datetime\").cast(\"timestamp\"))\n","\n","# Display cleansed data to destination\n","display(df)\n","\n","# Write cleansed data to destination\n","df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{lakehouse_silver}.{table_name}_cleansed\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"9ce7346c-9a47-413c-81bb-311e54813497"},{"cell_type":"markdown","source":["#### Clean data and add additional derived columns\n","\n","**<u>Add derived Columns</u>**\n","- pickupDate - convert datetime to date for visualizations and reporting.\n","- weekDay - day number of the week\n","- weekDayName - day names abbreviated.\n","- dayofMonth - day number of the month\n","- pickupHour - hour of pickup time\n","- trip_duration - representing duration in minutes of the trip.\n","- timeBins - Binned time of the day\n","\n","\n","**<u>Filter Conditions</u>** <p>\n","- fare_amount is between 0 and 100\n","- trip_distance greater than 0\n","- trip_duration is less than 3 hours (180 minutes)\n","- passenger_count is between 1 and 8.\n","- Remove outstation trips(outliers) trip_distance>100."],"metadata":{"collapsed":false},"id":"fc03e3ef-de71-4f46-9ae8-3cdc714a7ac0"},{"cell_type":"code","source":["from pyspark.sql.functions import col,when, dayofweek, date_format, hour,unix_timestamp, round, dayofmonth, lit\n","\n","nytaxidf_prep = df.withColumn('pickupDate', col('lpep_pickup_datetime').cast('date'))\\\n","                            .withColumn(\"weekDay\", dayofweek(col(\"lpep_pickup_datetime\")))\\\n","                            .withColumn(\"weekDayName\", date_format(col(\"lpep_pickup_datetime\"), \"EEEE\"))\\\n","                            .withColumn(\"dayofMonth\", dayofweek(col(\"lpep_pickup_datetime\")))\\\n","                            .withColumn(\"pickupHour\", hour(col(\"lpep_pickup_datetime\")))\\\n","                            .withColumn(\"trip_duration\", (unix_timestamp(col(\"lpep_dropoff_datetime\")) - unix_timestamp(col(\"lpep_pickup_datetime\")))/60)\\\n","                            .withColumn(\"timeBins\", when((col(\"pickupHour\") >=7) & (col(\"pickupHour\")<=10) ,\"MorningRush\")\\\n","                            .when((col(\"pickupHour\") >=11) & (col(\"pickupHour\")<=15) ,\"Afternoon\")\\\n","                            .when((col(\"pickupHour\") >=16) & (col(\"pickupHour\")<=19) ,\"EveningRush\")\\\n","                            .when((col(\"pickupHour\") <=6) | (col(\"pickupHour\")>=20) ,\"Night\"))\\\n","                            .filter(\"\"\"fare_amount > 0 AND fare_amount < 100 and trip_distance > 0 AND trip_distance < 100\n","                                    AND trip_duration > 0 AND trip_duration <= 180\n","                                    AND passenger_count > 0 AND passenger_count <= 8\"\"\")\n","\n","\n","# Write cleansed data to destination\n","nytaxidf_prep.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(f\"{lakehouse_silver}.{table_name}_cleansed\")"],"outputs":[],"execution_count":null,"metadata":{"collapsed":false},"id":"be9b07b1-4d19-4cce-9c7b-53900c8b5959"},{"cell_type":"markdown","source":["\n","Exploratory data analysis (EDA) is a common scenario for data engineers. EDA is the process of analyzing and understanding data to gain insights, identify patterns, and develop hypotheses for further investigation. In data engineering, EDA is often done to identify data quality issues, anomalies, or other problems that need to be addressed before data can be used for analysis or modeling. EDA can also help data engineers to understand the relationships between different data sources and determine the best way to join or transform them.\n","\n","`df.count()` is a Spark DataFrame API function that returns the number of rows in the DataFrame. It is a convenient way to quickly determine the size of the DataFrame without having to iterate over all the rows manually. The function is an action in Spark, meaning it triggers a computation that counts the number of rows in the DataFrame and returns the result. It is useful for getting a quick overview of the data size and checking if any rows are missing or dropped during data processing. However, it should be used with caution on large datasets, as it can be a costly operation that requires significant computational resources."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5446096d-fd17-463e-add5-74d3c3ab3039"},{"cell_type":"code","source":["# Load data from source\n","df = spark.sql(f\"SELECT * FROM {lakehouse_silver}.{table_name}_cleansed\")\n","\n","# Count the number of rows \n","df.count()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fd4b6b80-75b4-4f35-92e8-2863cf72d1bc"},{"cell_type":"markdown","source":["`df.dtypes` is an attribute of a DataFrame object that returns a list of tuples containing the column names and their corresponding data types. The data types are represented using the Spark SQL DataType class, which is a set of classes for representing data types in Spark SQL."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4f8a1891-f125-4708-ba40-5305300cc4e3"},{"cell_type":"code","source":["# Display the data types of the columns.\n","\n","df.dtypes"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0e0537e8-8720-449b-b722-3e9d44dbdc5e"},{"cell_type":"markdown","source":["The code imports the col function from `pyspark.sql.functions` and uses it to select the `\"vendorID\"` column from the Spark DataFrame `df`. The `groupBy()` function is then called on the resulting column object to group the DataFrame by the distinct values in the `\"vendorID\"` column. The `count()` function is then applied to the resulting grouped DataFrame to calculate the number of records in each group. Finally, the `show()` function is used to display the resulting DataFrame on the console."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"df7de758-ffdb-4d25-a4b5-3a420284b0bb"},{"cell_type":"code","source":["# Group the data by 'VendorID' and count the number of rows in each group. \n","\n","from pyspark.sql.functions import col\n","\n","df.groupBy(col(\"VendorID\")).count().show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2addfb21-025f-4f4e-8564-f226fbdf08a5"},{"cell_type":"markdown","source":["The code reads the Spark DataFrame `df` which contains information about NYC taxi trips. The code uses the `'min'` and `'max'` functions from PySpark to select the earliest and latest pickup dates respectively. These dates are stored in the variables `'oldest_day'` and `'latest_day'`. The `'collect'` function is then used to retrieve these values and they are printed to the console using the `'print'` function. The output displays the earliest and latest pickup dates in the dataset."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3e0e1d64-bdcf-4298-a1f2-303ce89c5840"},{"cell_type":"code","source":["# Retrieve information about the earliest and latest pickup dates in the dataset.\n","\n","from pyspark.sql.functions import min, max\n","\n","oldest_day = df.select(min(\"lpep_pickup_datetime\")).collect()[0][0]\n","latest_day = df.select(max(\"lpep_dropoff_datetime\")).collect()[0][0]\n","\n","print(\"Oldest pickup date: \", oldest_day)\n","print(\"Latest pickup date: \", latest_day)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9fa1b8b1-cf3d-414e-8bab-12a5aa5c6f57"},{"cell_type":"markdown","source":["This code uses the PySpark `date_format` function to group the `df` DataFrame by the year, month, and day of the `lpep_pickup_datetime` column, and then counts the number of occurrences for each date.\n","\n","`date_format` is a PySpark SQL function used to format the date or timestamp column to the specified format. In this code, the format used is `yyyy-MM-dd`. The alias `pickup_date` is assigned to the formatted date column, and the DataFrame is grouped by this column using the `groupby()` method. The `count()` method is then applied to count the number of occurrences of each pickup_date. Finally, the result is displayed using the `show()` method."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1bb290c5-2c08-4049-9b44-c7ced6ed44fa"},{"cell_type":"code","source":["from pyspark.sql.functions import date_format\n","\n","# group by year, month and day of lpepPickupDatetime\n","df_grouped = df.groupby(date_format('lpep_pickup_datetime', 'yyyy-MM-dd').alias('pickup_date')).count()\n","\n","# show the result\n","df_grouped.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"bd72968e-cecc-4fc9-99fd-d4ee10babe65"},{"cell_type":"markdown","source":["This code computes the minimum and maximum values of the fare_amount column in the Spark DataFrame df. It uses the `min()` and `max()` functions from the `pyspark.sql.functions` module to compute the minimum and maximum values, respectively. The `alias()` method is used to rename the resulting columns as `\"min\"` and `\"max\"`. Finally, the `show()` method is used to display the resulting DataFrame with two columns `\"min\"` and `\"max\"`, showing the minimum and maximum values of the `fare_amount column`."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c00799e4-3197-4a4d-82a8-dd0596fe9b8f"},{"cell_type":"code","source":["# min max values of target feature \"fare_amount\"\n","\n","df.select(min('fare_amount').alias('min'), max('fare_amount').alias('max')).show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"bfc069eb-b462-4ab3-b125-421ba416fb82"},{"cell_type":"markdown","source":["This code is performing descriptive statistical analysis on the `\"fare_amount\"` column of a Spark DataFrame named `\"df\"`. Specifically, it is using the `describe()` method of the DataFrame to compute summary statistics including `count`, `mean`, `standard deviation`, `minimum`, and `maximum`.\n","\n","The result of `describe()` is then converted to a Pandas DataFrame using the `toPandas()` method. This allows the statistics to be displayed in a more user-friendly table format, which includes the same summary statistics along with the 25th, 50th, and 75th percentiles. The resulting table provides insights into the central tendency and dispersion of the `\"fare_amount\"` variable, and can be useful for understanding the distribution of the data and identifying potential outliers."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"34bb841a-e110-457a-b9dd-8c60eb380a48"},{"cell_type":"code","source":["# General statistical characteristics of fare amount\n","\n","df.select('fare_amount').describe().toPandas()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5db90e8e-aaea-4b58-b7db-bfa4792aecc3"},{"cell_type":"markdown","source":["This code computes the approximate quantiles of the `'fare_amount'` column of the DataFrame `'df'` using the `'approxQuantile'` function from PySpark's SQL functions module. The function takes three arguments - the name of the column for which quantiles are to be computed, the list of quantile values to be returned, and a relative error value. In this case, the quantiles are 0.1, 0.25, 0.5, 0.75, and 0.9, and the relative error is set to 0.01. The function returns an array of approximate quantile values for the given column and quantile values."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"66506c32-5502-4532-9ffa-34a747a2b6c8"},{"cell_type":"code","source":["# quantiles\n","\n","df.select('fare_amount').approxQuantile(\"fare_amount\",[0.1, 0.25, 0.5, 0.75, 0.9], 0.01)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"54865cb8-b7d0-4439-9748-505fb91cf707"},{"cell_type":"markdown","source":["This code is used to plot the distribution of fare_amount using matplotlib library in Python. The fare_amount data is first extracted from the Spark DataFrame using the select function along with the F.col() function to extract the fare_amount column. The resulting DataFrame is then converted to a Pandas DataFrame using the toPandas() function. The fare_amount data is then plotted as a histogram using the hist() function from matplotlib. The number of bins for the histogram is set to 50 using the bins parameter. Finally, the title and axis labels for the plot are set using the title(), xlabel(), and ylabel() functions, and the plot is displayed using the show() function."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"df5fe23f-a0d9-4d5e-8bd7-04cb6a200fcf"},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import pyspark.sql.functions as F\n","\n","# Assuming your DataFrame is named `df`\n","fare_distribution = df.select(F.col('fare_amount')).toPandas()\n","\n","# Plot histogram\n","plt.hist(fare_distribution, bins=50)\n","plt.title('Distribution of Fare amount')\n","plt.xlabel('Fare amount')\n","plt.ylabel('Frequency')\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"42a85be6-1383-4424-9171-221aaec5357c"},{"cell_type":"markdown","source":["## Scatter chart\n","\n","This code snippet demonstrates how to create a scatter plot using Matplotlib in Python. The code assumes that the Spark DataFrame df contains the columns fare_amount and trip_distance. First, the Spark DataFrame is converted to a Pandas DataFrame using the toPandas() function. Then, a scatter plot is created using ax.scatter() function. The x and y arguments of the scatter() function represent the variables to be plotted on the x- and y-axes, respectively. The alpha argument controls the transparency of the points in the scatter plot. The axis labels and title are set using the ax.set_xlabel(), ax.set_ylabel(), and ax.set_title() functions. Finally, the plot is displayed using the plt.show() function. This code can be used to visualize the correlation between fare amount and trip distance in the DataFrame.\n","\n","To execute the cell code, use the shortcut CTRL + Enter on Windows, or ⌘ + Enter on MacOS. Alternatively, you can click the 'Run' icon (▶️) located on the left side of the code cell."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"62ef0ece-33f2-40a3-9961-cf3fb5113709"},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# convert Spark DataFrame to Pandas DataFrame\n","df_pd = df.select(['fare_amount', 'trip_distance']).toPandas()\n","\n","# create scatter plot\n","fig, ax = plt.subplots()\n","ax.scatter(x=df_pd['trip_distance'], y=df_pd['fare_amount'], alpha=0.5)\n","\n","# set axis labels and title\n","ax.set_xlabel('Trip Distance')\n","ax.set_ylabel('Fare Amount')\n","ax.set_title('Correlation between Fare Amount and Trip Distance')\n","\n","# show the plot\n","plt.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ade1e578-183d-4508-af8e-75b15475d3dd"},{"cell_type":"markdown","source":["# 2. Custom libraries & advanced visualisation"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"49943191-2ee8-43c8-bc0e-44fd64d67377"},{"cell_type":"markdown","source":["Libraries provide reusable code that Data Engineers may want to include in their Spark application. Each workspace comes with a pre-installed set of libraries available in the Spark run-time and available to be used immediately in the notebook or Spark job definition.  Based on the user scenarios and specific needs, you can include other libraries. There are two types of libraries you may want to include:\n","- Feed library: Feed libraries are the ones that come from public sources or repositories. You can install Python feed libraries from PyPI and Conda by specifying the source in the Library Management portals. You can also use a Conda environment specification .yml file to install libraries.\n","- Custom library: Custom libraries are the code built by you or your organization. .whl, .jar and .tar.gz can be managed through Library Management portals. Note that .tar.gz is only supported for R language, please use .whl for Python custom libraries."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c625ee42-e752-4ab4-bfe3-a1b84eeecfdd"},{"cell_type":"markdown","source":["## Install library\n","\n","The code line `pip install altair` is used to install the Python package \"Altair\" via the package manager \"pip\". Altair is a library for creating interactive visualizations in Python.\n","\n","\"Pip\" is a package manager for Python that allows users to easily install, manage, and update Python packages (libraries) from the Python Package Index (PyPI) and other package repositories. Pip can be used to install packages globally on the system or locally in a specific virtual environment.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"65e63ef6-283b-4cd4-b6ab-92e60495bfa3"},{"cell_type":"code","source":["pip install altair"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"117c437b-b3d1-423b-ab50-f22a78d63abd"},{"cell_type":"markdown","source":["## Create custom visualisation with a new library\n","\n","First, the code imports the Altair library using the alias \"alt\". Next, the code uses Spark SQL to select all columns from the \"NYC_Taxi_cleansed\" table in the \"Bronze\" database, limiting the result to the first 5000 rows. The resulting DataFrame is then converted to a Pandas DataFrame using the toPandas method.\n","\n","The alt.Chart method is then called with the Pandas DataFrame as the data source, and the mark_point method is used to specify that the chart should use points as the visual mark. The encode method is then used to specify the encoding for the x-axis, y-axis, and color of the points, as well as the tooltip values. The x-axis is mapped to the \"tripDistance\" column, the y-axis is mapped to the \"fareAmount\" column, and the color of the points is mapped to the \"paymentType\" column, which is treated as a categorical variable. The tooltip displays the \"tripDistance\", \"fareAmount\", and \"paymentType\" columns for each point.\n","\n","Finally, the interactive method is called to enable interactivity in the resulting visualization, allowing the user to zoom, pan, and view tooltip information when hovering over points in the scatter plot."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4a4ade8e-a962-4928-8986-207a83785044"},{"cell_type":"code","source":["import altair as alt\n","\n","df = spark.sql(f\"SELECT * FROM {lakehouse_bronze}.{table_name} LIMIT 5000\")\n","\n","data = df.toPandas()\n","\n","alt.Chart(data).mark_point().encode(\n","    x='trip_distance',\n","    y='fare_amount',\n","    color='payment_type:N',\n","    tooltip=['trip_distance', 'fare_amount', 'payment_type']\n",").interactive()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"f9fd9855-82fc-4106-b393-40fbe2ca9d98"},{"cell_type":"markdown","source":["# 3. Shortcuts and final table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b8e2ccb3-097a-49c5-aaee-2b7c71ab353f"},{"cell_type":"markdown","source":["Shortcuts in a lakehouse allow users to reference data without copying it. It unifies data from different lakehouses, workspaces, or external storage, such as ADLS Gen2 or AWS S3. You can quickly make large amounts of data available in your lakehouse locally without the latency of copying data from the source."],"metadata":{"nteract":{"transient":{"deleting":false}},"run_control":{"frozen":false},"editable":true},"id":"35f39c5b-c5ef-486e-b53d-653ea64edd87"},{"cell_type":"markdown","source":["## Load new data\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"14a63b91-323a-45e2-9766-0b63c9750bf1"},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/NYC-Taxi-Discounts-Per-Day.csv\")\n","# df now is a Spark DataFrame containing CSV data from \"Files/Generated-NYC-Taxi-Green-Discounts.csv\".\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"6f8b2314-25bb-4a41-b443-bfb556b5a221"},{"cell_type":"markdown","source":["## Unpivot sideloaded data\n","\n","The import pandas as pd line imports the Pandas library and assigns it an alias pd.\n","\n","Melt the discounts DataFrame: The pd.melt() function is used to convert the discouts_df PySpark DataFrame to a long format by converting date columns into rows. First, discouts_df.toPandas() is used to convert the PySpark DataFrame to a Pandas DataFrame. Then, pd.melt() takes the Pandas DataFrame, uses 'VendorID' as the identifier variable (id_vars), sets the 'date' as the variable name (var_name), and 'discount' as the value name (value_name). The melted DataFrame is stored in discouts_pd_df.\n","\n","Convert the melted DataFrame to a PySpark DataFrame: The spark.createDataFrame() function is used to convert the melted Pandas DataFrame discouts_pd_df back to a PySpark DataFrame, which is stored in the discounts_spark_df variable."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"dc80dbee-44ea-4c1d-9d44-e80022e062dd"},{"cell_type":"code","source":["import pandas as pd\n","\n","# Melt discouts_df to long format\n","discouts_pd_df = pd.melt(df.toPandas(), id_vars=['VendorID'], var_name='date', value_name='discount')\n","\n","discounts_spark_df = spark.createDataFrame(discouts_pd_df)\n","\n","display(discounts_spark_df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"895e1914-ed16-43f0-88e5-676f21ea294f"},{"cell_type":"markdown","source":["## Prepare data for join"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"70e900fd-7ccc-43a7-aead-5967c09a0f55"},{"cell_type":"code","source":["from pyspark.sql.functions import to_date\n","\n","nyc_taxi_df = spark.sql(f\"SELECT * FROM {lakehouse_bronze}.{table_name}\")\n","\n","nyc_taxi_df = nyc_taxi_df.withColumn(\"date\", to_date(\"lpep_pickup_datetime\"))\n","\n","display(nyc_taxi_df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"a12a1914-f098-400a-aa25-cb241a9d0c6f"},{"cell_type":"markdown","source":["## Join two datasets and save result"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3de18477-1750-4645-b281-b6385e5f6b4d"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Create aliases for your DataFrames\n","df1_alias = nyc_taxi_df.alias(\"df1\")\n","df2_alias = discounts_spark_df.alias(\"df2\")\n","\n","# Define the join condition using the aliases\n","join_condition = [col(\"df1.VendorID\") == col(\"df2.VendorID\"), col(\"df1.date\") == col(\"df2.date\")]\n","\n","# Perform the join using the aliases\n","result_df = df1_alias.join(df2_alias, join_condition, how='inner')  # You can use other join types like 'left', 'right', 'outer', etc.\n","\n","# Select only the desired columns\n","result_df = result_df.select(\"df1.VendorID\", \"df1.lpep_pickup_datetime\", \"df2.discount\")\n","\n","display(result_df)\n","\n","# Save the results to a new delta table\n","result_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{lakehouse_silver}.{table_name}_discounts\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"ae5f3e38-c05b-4c30-9ae6-c999a38d412d"},{"cell_type":"markdown","source":["## Refresh Lakehouse explorer  "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"374cb382-b2dc-4b28-9a9c-bad5e11bd7b6"},{"cell_type":"markdown","source":["# Now, continue with automation by proceeding to the next task (Exercise 2, Task 2.8)\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"883e082e-4c31-48c6-a048-6dbf9e6131cb"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"notebook_environment":{},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}